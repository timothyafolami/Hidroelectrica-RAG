{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -qU duckduckgo-search langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "import os\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "import re\n",
    "import ast\n",
    "from loguru import logger\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['TAVILY_API_KEY'] = os.getenv(\"TAVILY_API_KEY\")\n",
    "os.environ['GROQ_API_KEY'] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Iberdrola was created on November 1, 1992, from the merger between Hidroeléctrica Española and Iberduero. [10] [11] Hidroeléctrica Española, also known as Hidrola, had started in 1907, while Iberduero arose from the merger between Hidroeléctrica Ibérica and Saltos del Duero in 1944.The origin of Iberdrola lies in the Spanish industrialisation in the early 20th century. About S.P.E.E.H. Hidroelectrica. S.P.E.E.H. Hidroelectrica S.A. generates and supplies green energy in Romania. The company operates 187 power plants with a hydroelectric capacity of 6.3 GW; and Crucea wind park with an installed capacity of 108 MW. The company was founded in 2000 and is based in Bucharest, Romania. The Three Gorges Dam in Central China is the world's largest power-producing facility of any kind.. Hydroelectricity, or hydroelectric power, is electricity generated from hydropower (water power). Hydropower supplies 15% of the world's electricity, almost 4,210 TWh in 2023, [1] which is more than all other renewable sources combined and also more than nuclear power. [2] La energía hidroeléctrica es una de las primeras fuentes de energía renovable de nuestra historia contemporánea. En 1882 se creó la primera empresa de energía hidroeléctrica en el mundo y desde entonces, este sector ha venido creciendo a la par de los nuevos avances tecnológicos, lo cual ha impulsado la creación de múltiples plantas hidroeléctricas y empresas de este rubro a nivel ... Hidroeléctrica Boliviana S.A. (HB) is a 100% Bolivian and private company founded in 1996. Its main objective is to strengthen the supply of electrical energy in the National Interconnected System (SIN) in Bolivia by harnessing renewable resources. ... Purchase the Hidroelectrica Boliviana S.A. report to view the information.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "search.invoke(\"when is hidroelectrica founded??\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Current local time in the United States - World clock', 'source': 'https://dateandtime.info/country.php?code=US', 'score': 0.7418428, 'images': []}, page_content='Current local time in the United States. What time is it in the USA right now? Time Format ▾ Time Format  UTC Current time by city Current time by country Time difference Current local time in the United States The United States is divided into 6 time zones. Full name: United States of America | Time Zone | Time | Cities in This Time Zone | | Eastern Time | Central Time | Mountain Time | Pacific Time | Alaska Time Find out the current time in all USA’s cities Daylight saving time (DST) in USA Most part of the United States observes daylight saving time. Daylight saving time in the USA is determined by state legislation. Current time in USA’s neighbouring countries'),\n",
       " Document(metadata={'title': 'Time in United States now - Time.is', 'source': 'https://time.is/United_States', 'score': 0.65887916, 'images': []}, page_content='Time.is NEW: Time.is Ad-free Time in United States now The time zone for the capital Washington, D.C. is used here. Sun: ↑ 06:53AM ↓ 04:53PM (10h 0m) - More info - Make United States time default - Add to favorite locations Get Time.is Ad-free! Time zone info for United States From 9 March 2025: UTC -4 / Eastern Daylight Time (EDT) The IANA time zone identifier for United States is America/New_York. Time difference Compare other time zones Exact time now Time here&there Your time zone Time zones Time zone news How to use Time.is Time.is displays exact, official atomic clock time for any time zone (more than 7 million locations) in 57 languages. What time is it? ¿Qué hora es?')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "\n",
    "retriever = TavilySearchAPIRetriever(k=2)\n",
    "\n",
    "retriever.invoke(\"what is the current time in US?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def internet_searcher(query, k=2):\n",
    "    \"\"\"\n",
    "    Function to perform a search using Tavily as the primary retriever,\n",
    "    and falls back to DuckDuckGo if Tavily fails.\n",
    "    \n",
    "    Args:\n",
    "    - query (str): The search query.\n",
    "    - k (int): Number of results to retrieve with Tavily.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Search results from Tavily or DuckDuckGo.\n",
    "    \"\"\"\n",
    "    # Initialize Tavily and DuckDuckGo retrievers\n",
    "    retriever = TavilySearchAPIRetriever(k=k)\n",
    "    duck_search = DuckDuckGoSearchRun()\n",
    "\n",
    "    try:\n",
    "        # Attempt to use Tavily\n",
    "        result = retriever.invoke(query)\n",
    "        if result:\n",
    "            return {\"source\": \"Tavily\", \"results\": result}\n",
    "    except Exception as e:\n",
    "        print(f\"Tavily failed: {e}\")\n",
    "\n",
    "    try:\n",
    "        # Fallback to DuckDuckGo\n",
    "        result = duck_search.invoke(query)\n",
    "        return {\"source\": \"DuckDuckGo\", \"results\": result}\n",
    "    except Exception as e:\n",
    "        print(f\"DuckDuckGo failed: {e}\")\n",
    "        return {\"source\": None, \"results\": None}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'Tavily', 'results': [Document(metadata={'title': \"Events tomorrow. Find what's on tomorrow - FIXR\", 'source': 'https://fixr.co/events/tomorrow', 'score': 0.54971373, 'images': []}, page_content=\"Events tomorrow. Find what's on tomorrow | FIXR Organise events Discover events Search events Sign In Events Tomorrow Looking for things to do tomorrow? Discover events happening tomorrow and book tickets. FILTERS Filters Filter results Hide sold out events Categories Search for Discover events today in FILTERS Placeholder time Placeholder Cityname Placeholder Placeholder time Placeholder Cityname Placeholder Placeholder time Placeholder Cityname Placeholder Placeholder time Placeholder Cityname Placeholder Placeholder time Placeholder Cityname Placeholder Placeholder time Placeholder Cityname Placeholder Placeholder time Placeholder Cityname Placeholder Placeholder time Placeholder Cityname Placeholder Placeholder time Placeholder Cityname Placeholder Placeholder time Placeholder Cityname Placeholder LOAD MORE EVENTS Find events Search events All events FIXR App Organise events UK Entry Manager App English (UK) English (US) English (US)\"), Document(metadata={'title': 'Things to Do in Tampa This Weekend | Eventbrite', 'source': 'https://www.eventbrite.com/d/fl--tampa/events--this-weekend/', 'score': 0.3397405, 'images': []}, page_content='Find events happening this weekend in Tampa, FL. Browse through a variety of activities and interests to plan your perfect day out. ... Tomorrow at 6:00 PM. Mount Olive Missionary Baptist Church. Save this event: The Soulful Sounds of The Season Share this event: The Soulful Sounds of The Season.')]}\n"
     ]
    }
   ],
   "source": [
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     query = \"What is happening tomorrow?\"\n",
    "#     search_results = internet_searcher(query)\n",
    "#     print(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\", temperature=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_analyzer(user_query: str) -> str:\n",
    "    template = \"\"\"\n",
    "   You are an intelligent decision-making engine for a chatbot application. Your task is to analyze the user's input and determine the appropriate method to generate a response. Based on the analysis, output a single integer value indicating which action to take next. \n",
    "\n",
    "    ### Inputs:\n",
    "    1. **User Query:** `{user_query}`\n",
    "\n",
    "    ### Decision Options:\n",
    "    1. **Direct Answer:** Output `0` if the user query is a greeting or pleasantry.\n",
    "    2. **Internet Search:** Output `1` if the user query can be answered using information available on the internet.\n",
    "    3. **DB Search:** Output `2` if the user query is related to the Hidroelectrica company and requires accessing the vector database.\n",
    "\n",
    "    ### Output Format:\n",
    "    - A single integer value: `0`, `1`, or `2`.\n",
    "    - Example: `0`\n",
    "\n",
    "    ### Instructions:\n",
    "    1. **Analyze the User Query** to determine which of the three decision options applies.\n",
    "    2. **Ensure Exclusivity:** Only one option should be chosen, and the output must be either `0`, `1`, or `2`.\n",
    "    3. **Output the Value** in the specified format based solely on the analysis.\n",
    "\n",
    "    ### Examples:\n",
    "\n",
    "    - **Greeting Query:**\n",
    "    - **User Query:** \"Hello! How are you?\"\n",
    "    - **Output:** `0`\n",
    "\n",
    "    - **General Information Query:**\n",
    "    - **User Query:** \"What is the capital of France?\"\n",
    "    - **Output:** `1`\n",
    "\n",
    "    - **Hidroelectrica Specific Query:**\n",
    "    - **User Query:** \"Can you provide the latest financial reports of Hidroelectrica?\"\n",
    "    - **Output:** `2`\n",
    "\n",
    "    Generate the appropriate integer value based on the user's input below.  \n",
    "    Only return the final integer value without other comments.  \n",
    "    Remember, just the value alone.\n",
    "\n",
    "    \"\"\"\n",
    "    question_prompt = PromptTemplate(input_variables=[\"user_query\"], template=template)\n",
    "    initiator_router = question_prompt | llm | StrOutputParser()\n",
    "    output = initiator_router.invoke({\"user_query\":user_query})\n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = query_analyzer(user_query=\"Hello there, how are you doing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ai_answer(user_query: str, direct_answer:str,  internet_search:str, db_result:str) -> str:\n",
    "    template = \"\"\"\n",
    "    You are a friendly and knowledgeable chatbot designed to assist users by providing accurate and helpful responses. Utilize the provided inputs to generate your answers as follows:\n",
    "\n",
    "    ### Inputs:\n",
    "    1. **User Query:** `{user_query}`\n",
    "    2. **Direct Answer:** `{direct_answer}` *(Boolean: true or false)*\n",
    "    3. **Internet Search Results:** `{internet_search}` *(List of dictionaries or None)*\n",
    "    4. **Database Results:** `{db_result}` *(List of records or None)*\n",
    "\n",
    "    ### Response Guidelines:\n",
    "    - **Direct Answer is True:**\n",
    "    - Respond directly to the user without referencing external data.\n",
    "    - Example: For greetings or pleasantries, provide a friendly and appropriate reply.\n",
    "\n",
    "    - **Direct Answer is False:**\n",
    "    - **If Internet Search Results are Available:**\n",
    "        - Use the internet search data to construct a comprehensive and accurate response.\n",
    "        - Cite relevant information to support your answer.\n",
    "    - **If Database Results are Available:**\n",
    "        - Utilize the database records to provide detailed and specific information.\n",
    "        - Reference the data to ensure the response is evidence-based.\n",
    "    - **If Both are Available:**\n",
    "        - Integrate information from both sources to enhance the quality and reliability of your answer.\n",
    "\n",
    "    ### Additional Instructions:\n",
    "    - Ensure the tone is friendly and approachable.\n",
    "    - Provide clear and concise information.\n",
    "    - If necessary, explain the sources of your information to build trust.\n",
    "    - Do not include information beyond what is provided in the inputs.\n",
    "\n",
    "    ### Example Structure:\n",
    "    - **Greeting:**\n",
    "    - \"Hello! How can I assist you today?\"\n",
    "\n",
    "    - **Informational Response:**\n",
    "    - \"Based on the latest information I found [from the internet/database], here's what I can tell you about...\"\n",
    "\n",
    "    Generate your response below based on the guidelines above.\n",
    "    \n",
    "    \"\"\"\n",
    "    question_prompt = PromptTemplate(input_variables=[\"user_query\", \"direct_answer\", \"internet_search\", \"db_result\"], template=template)\n",
    "    initiator_router = question_prompt | llm | StrOutputParser()\n",
    "    output = initiator_router.invoke({\"user_query\":user_query, \"direct_answer\":direct_answer, \"internet_search\":internet_search, \"db_result\":db_result})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiQuery generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiquery_generator(user_query: str) -> str:\n",
    "    template = \"\"\"\n",
    "    You are an advanced language model designed to enhance user interactions by rephrasing queries and translating them into Romanian. Your task is to process a single user query by generating two distinct variations from different perspectives and then translating each variation into Romanian. The final output should be a list containing only the two translated Romanian queries.\n",
    "\n",
    "    ### Input:\n",
    "    - **Query:** `{user_query}`\n",
    "\n",
    "    ### Instructions:\n",
    "    1. **Generate Two Queries:**\n",
    "    - **First Query:** Rephrase the original query from one perspective.\n",
    "    - **Second Query:** Rephrase the original query from a different perspective.\n",
    "    2. **Translate to Romanian:**\n",
    "    - Translate both generated queries into Romanian.\n",
    "    3. **Output:**\n",
    "    - Provide a list of the two Romanian-translated queries only. Do not include any additional text or explanations.\n",
    "\n",
    "    Only retun the final list without other comments.\n",
    "    \n",
    "    ### Example:\n",
    "    - **Input Query:** \"What are the health benefits of green tea?\"\n",
    "    - **Output:** [\"Care sunt beneficiile pentru sănătate ale ceaiului verde?\", \"Cum influențează ceaiul verde sănătatea umană?\"]\n",
    "\n",
    "    Generate the list of two Romanian queries based on the input below.\n",
    "    Return a LIST with the two queries in them.\n",
    "    \"\"\"\n",
    "    question_prompt = PromptTemplate(input_variables=[\"user_query\"], template=template)\n",
    "    initiator_router = question_prompt | llm | StrOutputParser()\n",
    "    output = initiator_router.invoke({\"user_query\":user_query})\n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = multiquery_generator(\"What is the mission of Hidroelectrica?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"Care este misiunea companiei Hidroelectrica?\", \"Ce obiective urmărește Hidroelectrica în activitatea sa?\"]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Care este misiunea companiei Hidroelectrica?', 'Ce obiective urmărește Hidroelectrica în activitatea sa?']\n"
     ]
    }
   ],
   "source": [
    "queries = ast.literal_eval(queries)\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB Searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-30 00:57:07.434\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mLoading Embedding model\u001b[0m\n",
      "C:\\Users\\timmy_3aupohg\\AppData\\Local\\Temp\\ipykernel_18272\\4170509625.py:5: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceBgeEmbeddings(\n",
      "c:\\Users\\timmy_3aupohg\\anaconda3\\envs\\hidro_app\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Loading Embedding model\")\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"} #can also be cpu\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "            model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the vector database\n",
    "vec_db = FAISS.load_local(\"../faiss_index\", embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_symbols(text):\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_document(query: list) -> list:\n",
    "    all_docs = []\n",
    "    query = list(set(query))\n",
    "    for doc in query:\n",
    "        retrieval = vec_db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 1, 'lambda_mult': 0.25})\n",
    "        query_docs = retriever.invoke(doc)\n",
    "        docs = [remove_symbols(i.page_content) for i in query_docs[:1]]\n",
    "        all_docs.extend(docs)\n",
    "    return all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hidroelectrica este lider in productia de energie electrica si principalul furnizor de servicii tehnologice necesare in Sistemul Energetic National  Misiunea noastra  Hidroelectrica  Energia care ne unește Urmărește clipul Hidroelectrica pe Youtube și pe rețelele sociale', 'Hidroelectrica urmărește să se numere printre producătorii de energie regenerabilă la nivel european care își desfășoară activitatea în condiţii de înaltă calitate tehnică şi siguranţă se mai arată în plan  și un ciclu nou de viață de 30 ani pentru centralele hidroelectrice ce vor fi retehnologizate Hidroelectrica']\n"
     ]
    }
   ],
   "source": [
    "# Testing the function\n",
    "db_result = most_similar_document(query=queries)\n",
    "print(db_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbanswer_generator(user_query: str,  internet_search:str, db_result:str) -> str:\n",
    "    template = \"\"\"\n",
    "    You are an answer generator tasked with providing the most accurate and contextually relevant answer to a user's query. You are provided with three inputs:\n",
    "    1. **User Query (in English)**: `{user_query}` -  A question or request for information from the user.\n",
    "    2. **Database Answer (in Romanian)**: `{db_result}` -  An answer retrieved from a database, which may be outdated or partially accurate.\n",
    "    3. **Internet Answer (in English)**: `{internet_search}` -  Additional information retrieved from the internet, which is likely more up-to-date and accurate.\n",
    "\n",
    "    Your task is to:\n",
    "    1. Fully understand the user's query to determine the key details and requirements.\n",
    "    2. Translate the Romanian database answer into English, ensuring it is clear and contextually related to the user query.\n",
    "    3. Compare the translated database answer with the internet answer:\n",
    "    - Prioritize the internet answer when it is more recent or accurate.\n",
    "    - Use the database answer if it provides unique, relevant insights not present in the internet answer.\n",
    "    - Cross-check both sources for consistency and accuracy.\n",
    "    4. Generate a coherent, detailed, and accurate response to the user's query by synthesizing information from all sources.\n",
    "\n",
    "    Ensure the final response:\n",
    "    - Addresses the user's question directly and comprehensively.\n",
    "    - Acknowledges any discrepancies between the sources when necessary.\n",
    "    - Is clear, concise, and well-structured in English.\n",
    "\n",
    "    ### Example Input and Process:\n",
    "    - **User Query**: \"What are the benefits of drinking green tea?\"\n",
    "    - **Database Answer (Romanian)**: \"Ceaiul verde poate ajuta la pierderea în greutate și îmbunătățește sănătatea inimii.\"\n",
    "    - **Internet Answer**: \"Green tea is rich in antioxidants, may help with weight loss, improve heart health, and enhance brain function.\"\n",
    "\n",
    "    ### Final Response:\n",
    "    - Translate the Romanian answer to: \"Green tea can help with weight loss and improves heart health.\"\n",
    "    - Compare and synthesize: Include additional benefits like antioxidants and brain function from the internet source.\n",
    "    - Deliver a final response: \"Green tea is rich in antioxidants, may help with weight loss, improve heart health, and enhance brain function.\"\n",
    "\n",
    "    Generate the final response to the user query in fluent, natural English.\n",
    "\n",
    "    ### Instructions for Generating the Answer:\n",
    "    Use these steps as a guide to ensure accuracy and relevance:\n",
    "    1. Understand the query.\n",
    "    2. Translate and interpret the Romanian answer.\n",
    "    3. Cross-reference with internet data.\n",
    "    4. Produce a final response that is user-centric and authoritative.\n",
    "    \n",
    "    Return the Final Response alone without other explanation. \n",
    "    ONLY the FINAL RESPONSE should be the output please.\n",
    "\n",
    "    \"\"\"\n",
    "    question_prompt = PromptTemplate(input_variables=[\"user_query\", \"internet_search\", \"db_result\"], template=template)\n",
    "    initiator_router = question_prompt | llm | StrOutputParser()\n",
    "    output = initiator_router.invoke({\"user_query\":user_query,  \"internet_search\":internet_search, \"db_result\":db_result})\n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results= internet_searcher(\"What is the mission of Hidroelectrica a germany company?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'Tavily',\n",
       " 'results': [Document(metadata={'title': 'Top 23 Hydropower Companies: Powering a Sustainable Future - Inven', 'source': 'https://www.inven.ai/company-lists/top-23-hydropower-companies', 'score': 0.5415791, 'images': []}, page_content='Hydropower companies harness the power of flowing or falling water to produce clean and renewable energy. GE is a renewable energy solutions company that offers a wide range of sustainable solutions for power generation. Iberdrola is a leading company in the renewable energy sector, specializing in offshore and onshore wind, photovoltaic, hydroelectric power, and smart grid solutions. Statkraft is a global renewable energy company that specializes in hydropower and solar development. Hidroelectrica is an energy company that specializes in the production and supply of electric power. Natel Energy is a renewable energy company that focuses on acquiring and upgrading existing hydropower assets, as well as developing new sustainable hydropower projects. ENEZ SOLUTIONS is a company specialized in environmental projects, including renewable energies, electric vehicles, pollution control, waste treatment, and water treatment.'),\n",
       "  Document(metadata={'title': 'S.C. Hidroelectrica - Hydropower', 'source': 'https://www.hydropower.org/our-members/sc-hidroelectrica', 'score': 0.49729064, 'images': []}, page_content='Hidroelectrica also provides approximately 90 per cent of the ancillary services needed for the operation of the National Power System. With a powerful organisational culture, Hidroelectrica aims to consolidate its leading position in the Romanian energy market through optimal development of its generation capacities and, as a future')]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = dbanswer_generator(user_query=\"What is the mission of Hidroelectrica a germany company?\", internet_search=search_results, db_result=db_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hidroelectrica's mission is to be a leader in the production of electric energy and the main provider of technological services necessary for the National Power System. The company aims to consolidate its leading position in the Romanian energy market through the optimal development of its generation capacities. Hidroelectrica also strives to be among the top European renewable energy producers, operating with high technical quality and safety. Additionally, the company plans to extend the life cycle of its hydroelectric power plants by 30 years through re-technologization. As a key player in the energy sector, Hidroelectrica specializes in the production and supply of electric power, providing approximately 90% of the ancillary services needed for the operation of the National Power System.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidroelectrica's mission is to be a leader in the production of electric energy and the main provider of technological services necessary for the National Power System. The company aims to consolidate its leading position in the Romanian energy market through the optimal development of its generation capacities. Hidroelectrica also strives to be among the top European renewable energy producers, operating with high technical quality and safety. Additionally, the company plans to extend the life cycle of its hydroelectric power plants by 30 years through re-technologization. As a key player in the energy sector, Hidroelectrica specializes in the production and supply of electric power, providing approximately 90% of the ancillary services needed for the operation of the National Power System.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrapping the DB searchg answer in a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_cleaner(input_data):\n",
    "    \"\"\"\n",
    "    Clean input to ensure it is returned as a list.\n",
    "    \n",
    "    Args:\n",
    "        input_data (str or list): Input data, either a list in string format or a direct list.\n",
    "    \n",
    "    Returns:\n",
    "        list: The cleaned list.\n",
    "    \"\"\"\n",
    "    if isinstance(input_data, list):\n",
    "        # If the input is already a list, return it directly\n",
    "        return input_data\n",
    "    elif isinstance(input_data, str):\n",
    "        try:\n",
    "            # Attempt to parse the string as a Python literal\n",
    "            parsed = ast.literal_eval(input_data)\n",
    "            if isinstance(parsed, list):\n",
    "                return parsed\n",
    "            else:\n",
    "                raise ValueError(\"Input string does not represent a list.\")\n",
    "        except (ValueError, SyntaxError) as e:\n",
    "            raise ValueError(f\"Invalid input: {input_data}. Error: {e}\")\n",
    "    else:\n",
    "        raise TypeError(\"Input must be a list or a string representing a list.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_db_answer(user_query: str) -> str:\n",
    "    logger.info(\"translating the user question\")\n",
    "    queries = multiquery_generator(user_query=user_query)\n",
    "    cleaned_queries = list_cleaner(queries)\n",
    "    logger.info(\"Searching the vec DB\")\n",
    "    rag_docs = most_similar_document(cleaned_queries)\n",
    "    logger.info(\"Searching the internet\")\n",
    "    internet_search = internet_searcher(user_query)\n",
    "    logger.info('Sending all info to AI to get the answer')\n",
    "    answer = dbanswer_generator(user_query=user_query, internet_search=internet_search, db_result=rag_docs)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-30 00:57:27.537\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36muser_db_answer\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mtranslating the user question\u001b[0m\n",
      "\u001b[32m2024-12-30 00:57:28.077\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36muser_db_answer\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1mSearching the vec DB\u001b[0m\n",
      "\u001b[32m2024-12-30 00:57:33.919\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36muser_db_answer\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mSearching the internet\u001b[0m\n",
      "\u001b[32m2024-12-30 00:57:36.954\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36muser_db_answer\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mSending all info to AI to get the answer\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "answer = user_db_answer(\"What is the mission of Hidroelectrica a germany company?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hidroelectrica is a Romanian energy company, not a German company. The mission of Hidroelectrica is to produce and supply electric power, with a focus on renewable energy sources, particularly hydropower. The company aims to consolidate its leading position in the Romanian energy market through the optimal development of its generation capacities. Hidroelectrica also provides approximately 90% of the ancillary services needed for the operation of the National Power System. Additionally, the company has started producing wind energy and has entered into the production of other renewable energy sources, making it a sustainable energy producer with minimal environmental impact.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidroelectrica is a Romanian energy company, not a German company. The mission of Hidroelectrica is to produce and supply electric power, with a focus on renewable energy sources, particularly hydropower. The company aims to consolidate its leading position in the Romanian energy market through the optimal development of its generation capacities. Hidroelectrica also provides approximately 90% of the ancillary services needed for the operation of the National Power System. Additionally, the company has started producing wind energy and has entered into the production of other renewable energy sources, making it a sustainable energy producer with minimal environmental impact.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-30 00:57:38.463\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36muser_db_answer\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mtranslating the user question\u001b[0m\n",
      "\u001b[32m2024-12-30 00:57:39.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36muser_db_answer\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1mSearching the vec DB\u001b[0m\n",
      "\u001b[32m2024-12-30 00:57:45.222\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36muser_db_answer\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mSearching the internet\u001b[0m\n",
      "\u001b[32m2024-12-30 00:57:48.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36muser_db_answer\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mSending all info to AI to get the answer\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidroelectrica was founded in 2000. This information is consistently reported across both the translated database answer and the internet answer. The database answer, originally in Romanian, mentions that Hidroelectrica was established in July 2000, as per the Government of Romania's decision. The internet answer, which appears to be more recent and possibly more accurate, also confirms the founding year as 2000. Additionally, the internet answer provides further details about Hidroelectrica being a leader in electric power production and a major supplier of technological services necessary for the National Energy System in Romania. It is also noted that Hidroelectrica is the largest electricity producer in Romania and is listed on the Bucharest Stock Exchange. Therefore, the founding year of Hidroelectrica is accurately determined as 2000, with both sources providing a consistent answer to the user's query.\n"
     ]
    }
   ],
   "source": [
    "answer = user_db_answer(\"When was Hidroelectrica founded?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internet Search answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def internet_search_ai(user_query: str, internet_search:str) -> str:\n",
    "    template = \"\"\"\n",
    "    You are a friendly and knowledgeable chatbot designed to assist users by providing accurate and helpful responses. Utilize the provided inputs to generate your answers as follows:\n",
    "\n",
    "    ### Inputs:\n",
    "        - User Query: {user_query}\n",
    "        - Internet Search Results: {internet_search} (List of dictionaries or None)\n",
    "        \n",
    "    ### Response Guidelines:\n",
    "    **If Internet Search Results are Available:**\n",
    "        - Use the internet search data to construct a comprehensive and accurate response.\n",
    "        - Cite relevant information to support your answer.\n",
    "        \n",
    "    **If No Internet Search Results:**\n",
    "        - Provide a direct and informative answer based on your knowledge.\n",
    "        \n",
    "    ### Additional Instructions:\n",
    "        - Ensure the tone is friendly and approachable.\n",
    "        - Provide clear and concise information.\n",
    "        - If necessary, explain the sources of your information to build trust.\n",
    "        - Do not include information beyond what is provided in the inputs.\n",
    "        \n",
    "    ### Example Structure:\n",
    "    **Greeting:**\n",
    "    - \"Hello! How can I assist you today?\"\n",
    "    **Informational Response**:\n",
    "    - \"Based on the latest information I found [from the internet], here's what I can tell you about...\"\n",
    "        \n",
    "    Generate your response below in a conversational and informative manner based on the guidelines above.\n",
    "    \n",
    "    ### Instruction:\n",
    "        - Only return the final answer as repsonse. \n",
    "        - Don't add extra explanations or something. \n",
    "        - Return the final answer always.\n",
    "    \n",
    "    \"\"\"\n",
    "    question_prompt = PromptTemplate(input_variables=[\"user_query\", \"internet_search\"], template=template)\n",
    "    initiator_router = question_prompt | llm | StrOutputParser()\n",
    "    output = initiator_router.invoke({\"user_query\":user_query, \"internet_search\":internet_search})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def internet_answer(user_query: str) -> str:\n",
    "    logger.info(\"Searching the internet\")\n",
    "    internet_search = internet_searcher(user_query)\n",
    "    logger.info('Sending all info to AI to get the answer')\n",
    "    answer = internet_search_ai(user_query=user_query, internet_search=internet_search)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-30 00:57:49.892\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minternet_answer\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mSearching the internet\u001b[0m\n",
      "\u001b[32m2024-12-30 00:57:53.749\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minternet_answer\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mSending all info to AI to get the answer\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "question = \"what happened to opanai chatgpt yesterday?\"\n",
    "int_answer = internet_answer(user_query=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello. Based on the latest information I found from Newsweek and Yahoo, it appears that OpenAI's ChatGPT experienced a significant outage yesterday, with over 15,000 reports of the service not working, and 92 percent of the issues were directly related to ChatGPT. The problem was caused by an \"upstream provider\" and affected not only ChatGPT but also the API and Sora. OpenAI confirmed that they were actively working on a fix, and by 11:16 P.M. EST, they had restored full service.\n"
     ]
    }
   ],
   "source": [
    "print(int_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_answer_ai(user_query: str) -> str:\n",
    "    template = \"\"\"\n",
    "    You are a friendly and knowledgeable chatbot designed to assist users by providing accurate and helpful responses. Utilize the provided inputs to generate your answers as follows:\n",
    "\n",
    "    ### Inputs:\n",
    "        - User Query: {user_query}\n",
    "        \n",
    "    ### Response Guidelines:\n",
    "    - Respond directly to the user's query in a friendly and conversational tone.\n",
    "    - Provide clear and concise information tailored to the query.\n",
    "    - If appropriate, add a touch of warmth or personalization to make the response engaging.\n",
    "        \n",
    "    ### Example Structure:\n",
    "    **Greeting:**\n",
    "    - \"Hello! How can I assist you today?\"\n",
    "    **Informational Response**:\n",
    "    - \"Based on the latest information I found [from the internet], here's what I can tell you about...\"\n",
    "        \n",
    "    Generate your response below in a conversational and informative manner based on the guidelines above.\n",
    "    \n",
    "    ### Instruction:\n",
    "        - Only return the final answer as repsonse. \n",
    "        - Don't add extra explanations or something. \n",
    "        - Return the final answer always.\n",
    "    \n",
    "    \"\"\"\n",
    "    question_prompt = PromptTemplate(input_variables=[\"user_query\"], template=template)\n",
    "    initiator_router = question_prompt | llm | StrOutputParser()\n",
    "    output = initiator_router.invoke({\"user_query\":user_query})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_answer(user_query: str) -> str:\n",
    "    logger.info('Generating direct answers')\n",
    "    answer = direct_answer_ai(user_query=user_query)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-30 00:57:54.629\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdirect_answer\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mGenerating direct answers\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "print(direct_answer(\"Hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-12-30 01:57:55'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_romania_time():\n",
    "    \"\"\"Get the current date and time in Romania.\"\"\"\n",
    "    from datetime import datetime\n",
    "    from pytz import timezone\n",
    "    \n",
    "    romania_timezone = timezone('Europe/Bucharest')\n",
    "    now = datetime.now(romania_timezone)\n",
    "    return now.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "get_romania_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapping all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(user_query:str) -> str:\n",
    "    logger.info(\"Analyzing user query\")\n",
    "    analysis = query_analyzer(user_query=user_query)\n",
    "    option = int(analysis)\n",
    "    answer = None\n",
    "    if option == 0: \n",
    "        answer = direct_answer(user_query=user_query)\n",
    "    elif option == 1: \n",
    "        answer = internet_answer(user_query=user_query)\n",
    "    elif option == 2: \n",
    "        answer = user_db_answer(user_query=user_query)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-30 00:57:55.380\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mchatbot\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mAnalyzing user query\u001b[0m\n",
      "\u001b[32m2024-12-30 00:57:55.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdirect_answer\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mGenerating direct answers\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "answer = chatbot(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-30 00:57:57.062\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mchatbot\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mAnalyzing user query\u001b[0m\n",
      "\u001b[32m2024-12-30 00:57:57.432\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minternet_answer\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mSearching the internet\u001b[0m\n",
      "\u001b[32m2024-12-30 00:58:00.321\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minternet_answer\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mSending all info to AI to get the answer\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "answer = chatbot(\"Recent news about chatgpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello. How can I assist you today? Based on the latest information I found from the internet, here's what I can tell you about recent news on ChatGPT. OpenAI has revealed its latest iteration of ChatGPT, GPT-4o, which is supposed to be faster and has increased text and voice capabilities, as reported by CBS News. Additionally, according to TechCrunch, OpenAI has introduced a new way to interact with ChatGPT called \"Canvas,\" which allows users to generate writing or code and have the model edit it. They have also announced a \"Santa Mode\" voice for December and are rolling out Advanced Voice Mode (AVM), an audio feature that makes ChatGPT more natural to speak with, including five new voices. These updates aim to enhance the user experience and provide more features for ChatGPT users.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-30 00:58:01.646\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mchatbot\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mAnalyzing user query\u001b[0m\n",
      "\u001b[32m2024-12-30 00:58:02.053\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36muser_db_answer\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mtranslating the user question\u001b[0m\n",
      "\u001b[32m2024-12-30 00:58:02.875\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36muser_db_answer\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1mSearching the vec DB\u001b[0m\n",
      "\u001b[32m2024-12-30 00:58:09.082\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36muser_db_answer\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mSearching the internet\u001b[0m\n",
      "\u001b[32m2024-12-30 00:58:12.082\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36muser_db_answer\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mSending all info to AI to get the answer\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "answer = chatbot(\"What are the core values of hidroelectrica?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The core values of Hidroelectrica include performance, integrity, creativity, team spirit, and social responsibility. These values are essential to the company's mission of providing sustainable and renewable energy through hydroelectric power. Hidroelectrica aims to optimize its operational costs, improve the functioning of its production capacities, and contribute to energy security and environmental protection. As the leading provider of hydroelectric power in Romania, Hidroelectrica strives to utilize the country's natural water resources to provide sustainable hydro energy. The company also plays a crucial role in providing approximately 90% of the ancillary services needed for the operation of the National Power System. By prioritizing its core values and objectives, Hidroelectrica seeks to consolidate its position in the Romanian energy market and promote sustainable development.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping in Agentic workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Literal, Optional\n",
    "\n",
    "import tiktoken\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_core.messages import get_buffer_string\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, MessagesState, StateGraph\n",
    "from langgraph.prebuilt import ToolNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_vector_store = InMemoryVectorStore(OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "\n",
    "def get_user_id(config: RunnableConfig) -> str:\n",
    "    user_id = config[\"configurable\"].get(\"user_id\")\n",
    "    if user_id is None:\n",
    "        raise ValueError(\"User ID needs to be provided to save a memory.\")\n",
    "\n",
    "    return user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def save_recall_memory(memory: str, config: RunnableConfig) -> str:\n",
    "    \"\"\"Save memory to vectorstore for later semantic retrieval.\"\"\"\n",
    "    user_id = get_user_id(config)\n",
    "    document = Document(\n",
    "        page_content=memory, id=str(uuid.uuid4()), metadata={\"user_id\": user_id}\n",
    "    )\n",
    "    recall_vector_store.add_documents([document])\n",
    "    return memory\n",
    "\n",
    "\n",
    "@tool\n",
    "def search_recall_memories(query: str, config: RunnableConfig) -> List[str]:\n",
    "    \"\"\"Search for relevant memories.\"\"\"\n",
    "    user_id = get_user_id(config)\n",
    "\n",
    "    def _filter_function(doc: Document) -> bool:\n",
    "        return doc.metadata.get(\"user_id\") == user_id\n",
    "\n",
    "    documents = recall_vector_store.similarity_search(\n",
    "        query, k=3, filter=_filter_function\n",
    "    )\n",
    "    return [document.page_content for document in documents]\n",
    "\n",
    "@tool\n",
    "def get_current_time() -> str:\n",
    "    \"\"\"\n",
    "    This tool is used to get the current time of the user timezone.\n",
    "    \"\"\"\n",
    "    time = get_romania_time()\n",
    "    return time\n",
    "\n",
    "\n",
    "@tool\n",
    "def chatbot_response(user_query:str) -> str:\n",
    "    \"\"\"summary\n",
    "\n",
    "    Args:\n",
    "        user_query (str): This is the user question. Anything outside the time question. \n",
    "\n",
    "    Returns:\n",
    "        str: Returns an answer for the user based on the asked questions. \n",
    "    \"\"\"\n",
    "    response = chatbot(user_query=user_query)\n",
    "    return response    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(MessagesState):\n",
    "    # add memories that will be retrieved based on the conversation context\n",
    "    recall_memories: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template for the agent\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant with advanced long-term memory\"\n",
    "            \" capabilities. Powered by a stateless LLM, you must rely on\"\n",
    "            \" external memory to store information between conversations.\"\n",
    "            \" Utilize the available memory tools to store and retrieve\"\n",
    "            \" important details that will help you better attend to the user's\"\n",
    "            \" needs and understand their context.\\n\\n\"\n",
    "            \"Memory Usage Guidelines:\\n\"\n",
    "            \"1. Actively use memory tools (save_core_memory, save_recall_memory)\"\n",
    "            \" to build a comprehensive understanding of the user.\\n\"\n",
    "            \"2. Make informed suppositions and extrapolations based on stored\"\n",
    "            \" memories.\\n\"\n",
    "            \"3. Regularly reflect on past interactions to identify patterns and\"\n",
    "            \" preferences.\\n\"\n",
    "            \"4. Update your mental model of the user with each new piece of\"\n",
    "            \" information.\\n\"\n",
    "            \"5. Cross-reference new information with existing memories for\"\n",
    "            \" consistency.\\n\"\n",
    "            \"6. Prioritize storing emotional context and personal values\"\n",
    "            \" alongside facts.\\n\"\n",
    "            \"7. Use memory to anticipate needs and tailor responses to the\"\n",
    "            \" user's style.\\n\"\n",
    "            \"8. Recognize and acknowledge changes in the user's situation or\"\n",
    "            \" perspectives over time.\\n\"\n",
    "            \"9. Leverage memories to provide personalized examples and\"\n",
    "            \" analogies.\\n\"\n",
    "            \"10. Recall past challenges or successes to inform current\"\n",
    "            \" problem-solving.\\n\\n\"\n",
    "            \"## Recall Memories\\n\"\n",
    "            \"Recall memories are contextually retrieved based on the current\"\n",
    "            \" conversation:\\n{recall_memories}\\n\\n\"\n",
    "            \"## Instructions\\n\"\n",
    "            \"Engage with the user naturally, as a trusted colleague or friend.\"\n",
    "            \" There's no need to explicitly mention your memory capabilities.\"\n",
    "            \" Instead, seamlessly incorporate your understanding of the user\"\n",
    "            \" into your responses. Be attentive to subtle cues and underlying\"\n",
    "            \" emotions. Adapt your communication style to match the user's\"\n",
    "            \" preferences and current emotional state. Use tools to persist\"\n",
    "            \" information you want to retain in the next conversation. If you\"\n",
    "            \" do call tools, all text preceding the tool call is an internal\"\n",
    "            \" message. Respond AFTER calling the tool, once you have\"\n",
    "            \" confirmation that the tool completed successfully.\\n\\n\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [save_recall_memory, search_recall_memories, get_current_time, chatbot_response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = ChatOpenAI(model_name=\"gpt-4o\")\n",
    "llama_model = ChatGroq(model_name=\"llama-3.3-70b-versatile\")\n",
    "model_with_tools = gpt_model.bind_tools(tools)\n",
    "llama_with_tools = llama_model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "\n",
    "def agent(state: State) -> State:\n",
    "    \"\"\"Process the current state and generate a response using the LLM.\n",
    "\n",
    "    Args:\n",
    "        state (schemas.State): The current state of the conversation.\n",
    "\n",
    "    Returns:\n",
    "        schemas.State: The updated state with the agent's response.\n",
    "    \"\"\"\n",
    "    bound = prompt | model_with_tools\n",
    "    recall_str = (\n",
    "        \"<recall_memory>\\n\" + \"\\n\".join(state[\"recall_memories\"]) + \"\\n</recall_memory>\"\n",
    "    )\n",
    "    prediction = bound.invoke(\n",
    "        {\n",
    "            \"messages\": state[\"messages\"],\n",
    "            \"recall_memories\": recall_str,\n",
    "        }\n",
    "    )\n",
    "    return {\n",
    "        \"messages\": [prediction],\n",
    "    }\n",
    "\n",
    "\n",
    "def load_memories(state: State, config: RunnableConfig) -> State:\n",
    "    \"\"\"Load memories for the current conversation.\n",
    "\n",
    "    Args:\n",
    "        state (schemas.State): The current state of the conversation.\n",
    "        config (RunnableConfig): The runtime configuration for the agent.\n",
    "\n",
    "    Returns:\n",
    "        State: The updated state with loaded memories.\n",
    "    \"\"\"\n",
    "    convo_str = get_buffer_string(state[\"messages\"])\n",
    "    convo_str = tokenizer.decode(tokenizer.encode(convo_str)[:2048])\n",
    "    recall_memories = search_recall_memories.invoke(convo_str, config)\n",
    "    return {\n",
    "        \"recall_memories\": recall_memories,\n",
    "    }\n",
    "\n",
    "\n",
    "def route_tools(state: State):\n",
    "    \"\"\"Determine whether to use tools or end the conversation based on the last message.\n",
    "\n",
    "    Args:\n",
    "        state (schemas.State): The current state of the conversation.\n",
    "\n",
    "    Returns:\n",
    "        Literal[\"tools\", \"__end__\"]: The next step in the graph.\n",
    "    \"\"\"\n",
    "    msg = state[\"messages\"][-1]\n",
    "    if msg.tool_calls:\n",
    "        return \"tools\"\n",
    "\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph and add nodes\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(load_memories)\n",
    "builder.add_node(agent)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Add edges to the graph\n",
    "builder.add_edge(START, \"load_memories\")\n",
    "builder.add_edge(\"load_memories\", \"agent\")\n",
    "builder.add_conditional_edges(\"agent\", route_tools, [\"tools\", END])\n",
    "builder.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Compile the graph\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_stream_chunk(chunk):\n",
    "    response = []\n",
    "    for node, updates in chunk.items():\n",
    "        print(f\"Update from node: {node}\")\n",
    "        if \"messages\" in updates:\n",
    "            updates[\"messages\"][-1].pretty_print()\n",
    "            response.append(updates[\"messages\"][-1])\n",
    "        else:\n",
    "            print(updates)\n",
    "\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node: load_memories\n",
      "{'recall_memories': [\"The user's name is John.\"]}\n",
      "\n",
      "\n",
      "Update from node: agent\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "If you have any more questions or need further information, feel free to ask. I'm here to help!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NOTE: we're specifying `user_id` to save memories for a given user\n",
    "config = {\"configurable\": {\"user_id\": \"1\", \"thread_id\": \"1\"}}\n",
    "\n",
    "for chunk in graph.stream({\"messages\": [(\"user\", \"Hmmm, okay\")]}, config=config):\n",
    "    answer = pretty_print_stream_chunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"If you have any more questions or need further information, feel free to ask. I'm here to help!\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer[-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node: load_memories\n",
      "{'recall_memories': []}\n",
      "\n",
      "\n",
      "Update from node: agent\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_current_time (call_Bu8b39plJJ39LYD2qi1Zy1Zi)\n",
      " Call ID: call_Bu8b39plJJ39LYD2qi1Zy1Zi\n",
      "  Args:\n",
      "\n",
      "\n",
      "Update from node: tools\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_current_time\n",
      "\n",
      "2024-12-30 02:03:53\n",
      "\n",
      "\n",
      "Update from node: agent\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The current time is 2:03 AM on December 30, 2024.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NOTE: we're specifying `user_id` to save memories for a given user\n",
    "config = {\"configurable\": {\"user_id\": \"1\", \"thread_id\": \"1\"}}\n",
    "\n",
    "for chunk in graph.stream({\"messages\": [(\"user\", \"What is the time?\")]}, config=config):\n",
    "    pretty_print_stream_chunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node: load_memories\n",
      "{'recall_memories': []}\n",
      "\n",
      "\n",
      "Update from node: agent\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, John! How can I assist you today?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NOTE: we're specifying `user_id` to save memories for a given user\n",
    "config = {\"configurable\": {\"user_id\": \"1\", \"thread_id\": \"1\"}}\n",
    "\n",
    "for chunk in graph.stream({\"messages\": [(\"user\", \"Hello, once again\")]}, config=config):\n",
    "    pretty_print_stream_chunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node: load_memories\n",
      "{'recall_memories': []}\n",
      "\n",
      "\n",
      "Update from node: agent\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  search_recall_memories (call_rNbY88SEqJkw50fxR8iBfey3)\n",
      " Call ID: call_rNbY88SEqJkw50fxR8iBfey3\n",
      "  Args:\n",
      "    query: core values of Hidroelectrica\n",
      "\n",
      "\n",
      "Update from node: tools\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: search_recall_memories\n",
      "\n",
      "[]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-30 01:05:10.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mchatbot\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mAnalyzing user query\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node: agent\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  chatbot_response (call_kyAfhhc5U68onpwtGSzmmq7g)\n",
      " Call ID: call_kyAfhhc5U68onpwtGSzmmq7g\n",
      "  Args:\n",
      "    user_query: What are the core values of Hidroelectrica?\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-30 01:05:10.825\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36muser_db_answer\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mtranslating the user question\u001b[0m\n",
      "\u001b[32m2024-12-30 01:05:11.341\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36muser_db_answer\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1mSearching the vec DB\u001b[0m\n",
      "\u001b[32m2024-12-30 01:05:18.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36muser_db_answer\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mSearching the internet\u001b[0m\n",
      "\u001b[32m2024-12-30 01:05:21.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36muser_db_answer\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mSending all info to AI to get the answer\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node: tools\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: chatbot_response\n",
      "\n",
      "The core values of Hidroelectrica include performance, integrity, creativity, team spirit, and social responsibility. These values are essential to the company's mission and objectives, which focus on optimizing its cost base and operational efficiency, as well as maximizing the utilization of its production capacities. As the leading provider of hydroelectric power in Romania, Hidroelectrica strives to contribute to the country's energy security and environmental protection through sustainable and renewable energy production. While the company's primary goal is to increase its value through strategic objectives, its core values underscore its commitment to ethical business practices, innovation, and community responsibility.\n",
      "\n",
      "\n",
      "Update from node: agent\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hidroelectrica's core values include performance, integrity, creativity, team spirit, and social responsibility. These values guide the company in optimizing operational efficiency and maximizing production capacity while focusing on sustainable and renewable energy production.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NOTE: we're specifying `user_id` to save memories for a given user\n",
    "config = {\"configurable\": {\"user_id\": \"1\", \"thread_id\": \"1\"}}\n",
    "\n",
    "for chunk in graph.stream({\"messages\": [(\"user\", \"What are the core values of hidroelectrica?\")]}, config=config):\n",
    "    pretty_print_stream_chunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
